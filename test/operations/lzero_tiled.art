fn @get_accelerator () {
    levelzero_accelerator(0)
}

//TODO: these should not be needed. But being polymorphic in tensor element type breaks artic and thorin right now.
//These are copies taken from backend_gpu.art, replacing the tile sizes (which is fixable by making them an element of ops) and the Tensor[f16] type for c and d.
//Also: [[acc_type * y] * x] and the corresponding accesses are not supported, so lambdas are used for that. This should be ported to backend_gpu.art probably.
//Warning: The current solution requires the loops over tile_x and tile_y to be unrolled. Otherwise the program compiles and runs but results are incorrect ðŸ˜³
fn matrix_multiply_acc_tiled_intel [mat_a_type, mat_b_type, acc_type] (acc : Accelerator, ops : WMMAOperations[mat_a_type, mat_b_type, acc_type], a : Tensor[f16], b : Tensor[f16], c : Tensor[f32], d : Tensor[f32]) -> () {
    let m = a.y_dim;
    let n = b.x_dim;
    //assert(a.x_dim == b.y_dim);
    let k = a.x_dim;

    let tile_size_x = 8;
    let tile_size_y = 8;
    let tile_size_k = 16;

    //Each block will calculate x * y many tiles.
    let block_x_tiles = 8;
    let block_y_tiles = 8;

    //Each warp will be used to calculate this geometry of tiles.
    let warp_x_tiles = 2;
    let warp_y_tiles = 4;

    //Calculate the rest of the grid geometry
    let block_x_warps = block_x_tiles / warp_x_tiles;
    let block_y_warps = block_y_tiles / warp_y_tiles;

    let matrix_x_tiles = n / tile_size_x;
    let matrix_y_tiles = m / tile_size_y;

    let matrix_x_warps = matrix_x_tiles / warp_x_tiles;
    let matrix_y_warps = matrix_y_tiles / warp_y_tiles;

    let matrix_x_blocks = matrix_x_warps / block_x_warps;
    let matrix_y_blocks = matrix_y_warps / block_y_warps;

    let block_x_threads = block_x_warps * wave_size;
    let block_y_threads = block_y_warps;

    let matrix_x_threads = matrix_x_blocks * block_x_threads;
    let matrix_y_threads = matrix_y_blocks * block_y_threads;

    print_string("tiled configuration: (");
    print_i32(matrix_x_threads);
    print_string(", ");
    print_i32(matrix_y_threads);
    print_string(") block size (");
    print_i32(block_x_threads);
    print_string(", ");
    print_i32(block_y_threads);
    print_string(")\n");

    for work_item in acc.exec((matrix_x_threads, matrix_y_threads,  1), (block_x_threads, block_y_threads, 1)) {
        let warp_x = work_item.gidx() / wave_size * warp_x_tiles * tile_size_x;
        let warp_y = work_item.gidy() * warp_y_tiles * tile_size_y;

        //TODO: this depends on the current layout!
        //let mut acc_fragments : [[acc_type * 4] * 2];
        let mut acc_fragment_0 : acc_type;
        let mut acc_fragment_1 : acc_type;
        let mut acc_fragment_2 : acc_type;
        let mut acc_fragment_3 : acc_type;
        let mut acc_fragment_4 : acc_type;
        let mut acc_fragment_5 : acc_type;
        let mut acc_fragment_6 : acc_type;
        let mut acc_fragment_7 : acc_type;
        let acc_fragments = @|tile_x : i32| { @|tile_y : i32| {
            match (tile_x, tile_y) {
                (0, 0) => &mut acc_fragment_0,
                (0, 1) => &mut acc_fragment_1,
                (0, 2) => &mut acc_fragment_2,
                (0, 3) => &mut acc_fragment_3,
                (1, 0) => &mut acc_fragment_4,
                (1, 1) => &mut acc_fragment_5,
                (1, 2) => &mut acc_fragment_6,
                (1, 3) => &mut acc_fragment_7,
                _ => &mut acc_fragment_0
            }
        }};

        /* Load C fragments into registers */
        for tile_x in unroll(0, warp_x_tiles) {
            for tile_y in unroll(0, warp_y_tiles) {
                let local_x = tile_x * tile_size_x;
                let local_y = tile_y * tile_size_y;

                let global_x = warp_x + local_x;
                let global_y = warp_y + local_y;

                let c_fragment_tensor = sub_tensor(c, global_x, global_y, tile_size_x, tile_size_y);
                let c_cast = Tensor[f16] { data = bitcast[&mut [f16]](c_fragment_tensor.data), x_dim = 0, y_dim = 0, addr_mode = c_fragment_tensor.addr_mode, stride = c_fragment_tensor.stride };
                let c_fragment = ops.load_c(c_cast);

                *acc_fragments(tile_x)(tile_y) = c_fragment;
            }
        }

        for global_k in range_step(0, k, tile_size_k) {
            //let mut a_fragments : [mat_a_type * 4];
            let mut a_fragment_0 : mat_a_type;
            let mut a_fragment_1 : mat_a_type;
            let mut a_fragment_2 : mat_a_type;
            let mut a_fragment_3 : mat_a_type;
            let a_fragments = @|tile_y : i32| {
                match tile_y {
                    0 => &mut a_fragment_0,
                    1 => &mut a_fragment_1,
                    2 => &mut a_fragment_2,
                    3 => &mut a_fragment_3,
                    _ => &mut a_fragment_0
                }
            };

            for tile_x in unroll(0, warp_x_tiles /* = 2 */) {
                let local_x = tile_x * tile_size_x;
                let global_x = warp_x + local_x;

                let b_fragment_tensor = sub_tensor(b, global_x, global_k, tile_size_x, tile_size_k);
                let b_fragment = ops.load_b(b_fragment_tensor);

                for tile_y in unroll(0, warp_y_tiles /* = 4 */) {
                    let local_y = tile_y * tile_size_y;
                    let global_y = warp_y + local_y;

                    let acc_fragment = *acc_fragments(tile_x)(tile_y);

                    let a_fragment_tensor = sub_tensor(a, global_k, global_y, tile_size_k, tile_size_y);
                    let a_fragment = if (tile_x == 0) {
                        let a_fragment = ops.load_a(a_fragment_tensor);
                        *a_fragments(tile_y) = a_fragment;
                        a_fragment
                    } else {
                        *a_fragments(tile_y)
                    };

                    let result_fragment = ops.wmma (a_fragment, a_fragment_tensor.addr_mode, b_fragment, b_fragment_tensor.addr_mode, acc_fragment, c.addr_mode);

                    *acc_fragments(tile_x)(tile_y) = result_fragment;
                }
            }
        }

        for tile_x in unroll(0, warp_x_tiles) {
            for tile_y in unroll(0, warp_y_tiles) {
                let local_x = tile_x * tile_size_x;
                let local_y = tile_y * tile_size_y;

                let global_x = warp_x + local_x;
                let global_y = warp_y + local_y;

                let result_fragment = *acc_fragments(tile_x)(tile_y);

                let d_fragment_tensor = sub_tensor(d, global_x, global_y, tile_size_x, tile_size_y);
                let d_cast = Tensor[f16] { data = bitcast[&mut [f16]](d_fragment_tensor.data), x_dim = 0, y_dim = 0, addr_mode = d_fragment_tensor.addr_mode, stride = d_fragment_tensor.stride };

                ops.store_d (d_cast, result_fragment);
            }
        }
    }
}

fn matrix_multiply_acc_wmma_intel [mat_a_type, mat_b_type, acc_type] (acc : Accelerator, ops : WMMAOperations[mat_a_type, mat_b_type, acc_type], a : Tensor[f16], b : Tensor[f16], c : Tensor[f32], d : Tensor[f32]) -> () {
    let m = a.y_dim;
    let n = b.x_dim;
    //assert(a.x_dim == b.y_dim);
    let k = a.x_dim;

    let tile_size_x = 8;
    let tile_size_y = 8;
    let tile_size_k = 16;

    //We run blocks containing block_factor_x * block_factor_y many tiles.
    //let block_factor_x = select(n % (tile_size_x * 2) != 0, 1, select(n % (tile_size_x * 4) != 0, 2, 4));
    //let block_factor_y = select(m % (tile_size_y * 2) != 0, 1, select(m % (tile_size_y * 4) != 0, 2, 4));
    let block_factor_x = 1;
    let block_factor_y = 1;

    let matrix_x_threads = n * wave_size / tile_size_x;
    let matrix_y_threads = m / tile_size_y;

    let block_x_threads = wave_size * block_factor_x;
    let block_y_threads = block_factor_y;

    print_string("simple wmma configuration: (");
    print_i32(matrix_x_threads);
    print_string(", ");
    print_i32(matrix_y_threads);
    print_string(") block size (");
    print_i32(block_x_threads);
    print_string(", ");
    print_i32(block_y_threads);
    print_string(")\n");

    for work_item in acc.exec((matrix_x_threads, matrix_y_threads,  1), (block_x_threads, block_y_threads, 1)) {
        let tile_x = work_item.gidx() / wave_size * tile_size_x;
        let tile_y = work_item.gidy() * tile_size_y;

        let c_fragment_tensor = sub_tensor(c, tile_x, tile_y, tile_size_x, tile_size_y);
        let c_cast = Tensor[f16] { data = bitcast[&mut [f16]](c_fragment_tensor.data), x_dim = 0, y_dim = 0, addr_mode = c_fragment_tensor.addr_mode, stride = c_fragment_tensor.stride };
        let mut acc_fragment = ops.load_c(c_cast);

        for global_k in range_step(0, k, tile_size_k) {
            let a_fragment_tensor = sub_tensor(a, global_k, tile_y, tile_size_k, tile_size_y);
            let b_fragment_tensor = sub_tensor(b, tile_x, global_k, tile_size_x, tile_size_k);

            let a_fragment = ops.load_a(a_fragment_tensor);
            let b_fragment = ops.load_b(b_fragment_tensor);

            acc_fragment = ops.wmma(a_fragment, a_fragment_tensor.addr_mode, b_fragment, b_fragment_tensor.addr_mode, acc_fragment, c_fragment_tensor.addr_mode);
        }

        let d_fragment_tensor = sub_tensor(d, tile_x, tile_y, tile_size_x, tile_size_y);
        let d_cast = Tensor[f16] { data = bitcast[&mut [f16]](d_fragment_tensor.data), x_dim = 0, y_dim = 0, addr_mode = d_fragment_tensor.addr_mode, stride = d_fragment_tensor.stride };
        ops.store_d(d_cast, acc_fragment);
    }
}

fn @accelerated_matmul (lzero : Accelerator, a : Tensor[f16], b : Tensor[f16], c : Tensor[f16], d : Tensor[f16]) -> () {
    let num_elements = (c.x_dim * c.y_dim) as i64;

    let c_f32_buffer = lzero.alloc(sizeof[f32]() * num_elements);
    let d_f32_buffer = lzero.alloc(sizeof[f32]() * num_elements);

    let c_f32 = Tensor[f32] { data = bitcast[&mut[f32]](c_f32_buffer.data), x_dim = c.x_dim, y_dim = c.y_dim, addr_mode = c.addr_mode, stride = c.stride };
    let d_f32 = Tensor[f32] { data = bitcast[&mut[f32]](d_f32_buffer.data), x_dim = d.x_dim, y_dim = d.y_dim, addr_mode = d.addr_mode, stride = d.stride };

    for work_item in lzero.exec((c.x_dim, c.y_dim, 1), (1, 1, 1)) {
        c_f32.data(addr_tensor(work_item.gidx(), work_item.gidy(), c_f32)) = c.data(addr_tensor(work_item.gidx(), work_item.gidy(), c)) as f32;
    }

    if (M % 64 == 0 && N % 64 == 0) {
        matrix_multiply_acc_tiled_intel [spv_mat_a_datatype, spv_mat_b_datatype, spv_acc_datatype] (lzero, SPVWMMAOperations, a, b, c_f32, d_f32);
    } else {
        matrix_multiply_acc_wmma_intel [spv_mat_a_datatype, spv_mat_b_datatype, spv_acc_datatype] (lzero, SPVWMMAOperations, a, b, c_f32, d_f32);
    }

    for work_item in lzero.exec((d.x_dim, d.y_dim, 1), (1, 1, 1)) {
        d.data(addr_tensor(work_item.gidx(), work_item.gidy(), d)) = d_f32.data(addr_tensor(work_item.gidx(), work_item.gidy(), d_f32)) as f16;
    }
}

fn @ref_matmul (a : Tensor[f16], b : Tensor[f16], c : Tensor[f16], d : Tensor[f16]) -> () {
    for x in range(0, c.x_dim) {
        for y in range(0, c.y_dim) {
            d.data(addr_tensor(x, y, d)) = c.data(addr_tensor(x, y, d));
        }
    }

    matrix_multiply_blas(a, b, d);
}
