type a_element_type = f16;
type b_element_type = f16;
type c_element_type = f16;

static tile_size_x = 16;
static tile_size_y = 16;
static tile_size_k = 16;

static mut device_mp_count : i32;

fn @get_accelerator () {
    device_mp_count = runtime_device_nodes(runtime_device(1, 0));
    amdgpu_hsa_accelerator(1)
}

fn @accelerated_matmul (hsa : Accelerator, a : Tensor[a_element_type], b : Tensor[b_element_type], c : Tensor[c_element_type], d : Tensor[c_element_type]) -> () {
    if (M % 128 == 0 && N % 128 == 0) {
        matrix_multiply_acc_tiled [amdgcn_mat_datatype, amdgcn_mat_datatype, amdgcn_acc_datatype] (hsa, HSAWMMAOperations, a, b, c, d);
    } else {
        matrix_multiply_acc_wmma [amdgcn_mat_datatype, amdgcn_mat_datatype, amdgcn_acc_datatype] (hsa, HSAWMMAOperations, a, b, c, d);
    }
}

fn @ref_matmul (a : Tensor[a_element_type], b : Tensor[b_element_type], c : Tensor[c_element_type], d : Tensor[c_element_type]) -> () {
    for x in range(0, c.x_dim) {
        for y in range(0, c.y_dim) {
            d.data(addr_tensor(x, y, d)) = c.data(addr_tensor(x, y, d));
        }
    }

    matrix_multiply_blas(a, b, d);
}
