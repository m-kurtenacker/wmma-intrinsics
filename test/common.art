/*
 * D = A * B + C
 *
 * Where:
 *
 * A:    (  -> K )
 *       (       )
 *       (| M    )
 *       (V      )
 *
 * B:    (  -> N )
 *       (       )
 *       (| K    )
 *       (V      )
 *
 * C, D: (  -> N )
 *       (       )
 *       (| M    )
 *       (V      )
 */

#[export]
fn main (_argc : i32, _argv : &[&[u8]]) -> i32 {
    let nvvm = nvvm_accelerator(0);

    let astride = match alayout { AddrMode::RowMayor => K, AddrMode::ColMayor => M };
    let bstride = match blayout { AddrMode::RowMayor => N, AddrMode::ColMayor => K };
    let cstride = match clayout { AddrMode::RowMayor => N, AddrMode::ColMayor => M };

    // Produce data set on host
    let a_cpu_buffer = alloc_cpu(sizeof[f16]() * (K * M) as i64);
    let b_cpu_buffer = alloc_cpu(sizeof[f16]() * (N * K) as i64);
    let c_cpu_buffer = alloc_cpu(sizeof[f16]() * (N * M) as i64);
    let d_cpu_buffer = alloc_cpu(sizeof[f16]() * (N * M) as i64);
    let r_cpu_buffer = alloc_cpu(sizeof[f16]() * (N * M) as i64);

    let a_cpu = bitcast[&mut [f16]](a_cpu_buffer.data);
    let b_cpu = bitcast[&mut [f16]](b_cpu_buffer.data);
    let c_cpu = bitcast[&mut [f16]](c_cpu_buffer.data);
    let d_cpu = bitcast[&mut [f16]](d_cpu_buffer.data);
    let r_cpu = bitcast[&mut [f16]](r_cpu_buffer.data);

    for n in range(0, M * K) { a_cpu(n) = random_val_f32() as f16; }
    for n in range(0, N * K) { b_cpu(n) = random_val_f32() as f16; } 
    for n in range(0, N * M) { c_cpu(n) = random_val_f32() as f16; d_cpu(n) = 0 as f16; r_cpu(n) = 0 as f16; }

    // Copy data to device
    let a_cuda_buffer = nvvm.alloc(sizeof[f16]() * (K * M) as i64);
    let b_cuda_buffer = nvvm.alloc(sizeof[f16]() * (N * K) as i64);
    let c_cuda_buffer = nvvm.alloc(sizeof[f16]() * (N * M) as i64);
    let d_cuda_buffer = nvvm.alloc(sizeof[f16]() * (N * M) as i64);

    copy(a_cpu_buffer, a_cuda_buffer);
    copy(b_cpu_buffer, b_cuda_buffer);
    copy(c_cpu_buffer, c_cuda_buffer);

    let a_global_tensor = Tensor_global { data = bitcast[&mut addrspace(1)[f16]](a_cuda_buffer.data), x_dim = K, y_dim = M, addr_mode = alayout, stride = astride };
    let b_global_tensor = Tensor_global { data = bitcast[&mut addrspace(1)[f16]](b_cuda_buffer.data), x_dim = N, y_dim = K, addr_mode = blayout, stride = bstride };
    let c_global_tensor = Tensor_global { data = bitcast[&mut addrspace(1)[f16]](c_cuda_buffer.data), x_dim = N, y_dim = M, addr_mode = clayout, stride = cstride };
    let d_global_tensor = Tensor_global { data = bitcast[&mut addrspace(1)[f16]](d_cuda_buffer.data), x_dim = N, y_dim = M, addr_mode = clayout, stride = cstride };

    // NVVM multiply tensors
    matrix_multiply_nvvm(nvvm, a_global_tensor, b_global_tensor, c_global_tensor, d_global_tensor);

    // Fetch result
    copy(d_cuda_buffer, d_cpu_buffer);

    // Ref implementation
    let a_tensor = Tensor { data = a_cpu, x_dim = K, y_dim = M, addr_mode = alayout, stride = astride };
    let b_tensor = Tensor { data = b_cpu, x_dim = N, y_dim = K, addr_mode = blayout, stride = bstride };
    let c_tensor = Tensor { data = c_cpu, x_dim = N, y_dim = M, addr_mode = clayout, stride = cstride };
    let r_tensor = Tensor { data = r_cpu, x_dim = N, y_dim = M, addr_mode = clayout, stride = cstride };
    matrix_multiply_naive(a_tensor, b_tensor, c_tensor, r_tensor);

    // Compare results
    let fin_tensor = Tensor { data = r_cpu, x_dim = N, y_dim = M, addr_mode = clayout, stride = cstride };
    let d_tensor = Tensor { data = d_cpu, x_dim = N, y_dim = M, addr_mode = clayout, stride = cstride };

    print_string("Fin matrix:\n");
    print_matrix(fin_tensor);
    print_string("\n");
    print_string("D matrix:\n");
    print_matrix(d_tensor);
    print_string("\n");

    for x in range(0, d_tensor.x_dim) {
        for y in range(0, d_tensor.y_dim) {
            fin_tensor.data(addr_tensor(x, y, fin_tensor)) = fin_tensor.data(addr_tensor(x, y, fin_tensor)) - d_tensor.data(addr_tensor(x, y, d_tensor));
        }
    }

    print_string("Difference:\n");
    print_matrix(fin_tensor);
    print_string("\n");

    let mut max_error = 0 as f32;
    for x in range(0, N * M) {
        max_error = cpu_intrinsics.fmaxf(max_error, cpu_intrinsics.fabsf(r_cpu(x) as f32));
    }

    if (max_error < 0.05) {
        0
    } else {
        print_string("\n");
        print_string("Max error: ");
        print_f32(max_error);
        print_string("\n");
        1
    }
}
