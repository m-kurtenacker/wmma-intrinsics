/*
 * D = A * B + C
 *
 * Where:
 *
 * A:    (  -> K )
 *       (       )
 *       (| M    )
 *       (V      )
 *
 * B:    (  -> N )
 *       (       )
 *       (| K    )
 *       (V      )
 *
 * C, D: (  -> N )
 *       (       )
 *       (| M    )
 *       (V      )
 */

//Tested with ROCm 6.3.3
//TODO: This requires a patch in runtime:
/* in hsa_platform.cpp:616
-    std::string attrs = "-trap-handler";
+    std::string attrs = "-trap-handler,+wavefrontsize32,-wavefrontsize64";
*/

// This should never changeâ„¢.
static wave_size = 32;
static device_mp_count = 20; //TODO: Turn that into an intrinsic or something?

struct WMMAOperations [mat_a_type, mat_b_type, acc_type] {
    load_a : fn (Tensor) -> mat_a_type,
    load_b : fn (Tensor) -> mat_b_type,
    load_c : fn (Tensor_f32) -> acc_type,
    wmma : fn (mat_a_type, AddrMode, mat_b_type, AddrMode, acc_type, AddrMode) -> acc_type,
    store_d : fn (Tensor_f32, acc_type) -> ()
}

/* Improved tiled version. We split the matrix in 16x16 tiles, and combine 8x8 such tiles to one block.
   Each block will then be computed using 8 warps. Consequently, each warp will compute 8 tiles. */
fn matrix_multiply_acc_tiled [mat_a_type, mat_b_type, acc_type] (acc : Accelerator, ops : WMMAOperations[mat_a_type, mat_b_type, acc_type], a : Tensor, b : Tensor, c : Tensor_f32, d : Tensor_f32) -> () {
    let m = a.y_dim;
    let n = b.x_dim;
    //assert(a.x_dim == b.y_dim);
    let k = a.x_dim;

    let tile_size_x = 8;
    let tile_size_y = 8;
    let tile_size_k = 16;

    //Each block will calculate x * y many tiles.
    let block_x_tiles = 8;
    let block_y_tiles = 8;

    //Each warp will be used to calculate this geometry of tiles.
    let warp_x_tiles = 2;
    let warp_y_tiles = 4;

    //Calculate the rest of the grid geometry
    let block_x_warps = block_x_tiles / warp_x_tiles;
    let block_y_warps = block_y_tiles / warp_y_tiles;

    let matrix_x_tiles = n / tile_size_x;
    let matrix_y_tiles = m / tile_size_y;

    let matrix_x_warps = matrix_x_tiles / warp_x_tiles;
    let matrix_y_warps = matrix_y_tiles / warp_y_tiles;

    let matrix_x_blocks = matrix_x_warps / block_x_warps;
    let matrix_y_blocks = matrix_y_warps / block_y_warps;

    let block_x_threads = block_x_warps * wave_size;
    let block_y_threads = block_y_warps;

    let matrix_x_threads = matrix_x_blocks * block_x_threads;
    let matrix_y_threads = matrix_y_blocks * block_y_threads;

    print_string("tiled configuration: (");
    print_i32(matrix_x_threads);
    print_string(", ");
    print_i32(matrix_y_threads);
    print_string(") block size (");
    print_i32(block_x_threads);
    print_string(", ");
    print_i32(block_y_threads);
    print_string(")\n");

    for work_item in acc.exec((matrix_x_threads, matrix_y_threads,  1), (block_x_threads, block_y_threads, 1)) {
        let warp_x = work_item.gidx() / wave_size * warp_x_tiles * tile_size_x;
        let warp_y = work_item.gidy() * warp_y_tiles * tile_size_y;

        //TODO: this depends on the current layout!
        //let mut acc_fragments : [[acc_type * 2] * 2];
        let mut acc_fragment_1 : acc_type;
        let mut acc_fragment_2 : acc_type;
        let mut acc_fragment_3 : acc_type;
        let mut acc_fragment_4 : acc_type;
        let mut acc_fragment_5 : acc_type;
        let mut acc_fragment_6 : acc_type;
        let mut acc_fragment_7 : acc_type;
        let mut acc_fragment_8 : acc_type;

        let acc_fragments = @|x : i32| { @|y : i32| {
            match (x, y) {
                (0, 0) => &mut acc_fragment_1,
                (0, 1) => &mut acc_fragment_2,
                (0, 2) => &mut acc_fragment_3,
                (0, 3) => &mut acc_fragment_4,
                (1, 0) => &mut acc_fragment_5,
                (1, 1) => &mut acc_fragment_6,
                (1, 2) => &mut acc_fragment_7,
                (1, 3) => &mut acc_fragment_8,
                _ => { pe_info("acc fragment not found", (x, y)); &mut acc_fragment_1 }
            }
        }};

        /* Load C fragments into registers */
        for tile_x in unroll(0, warp_x_tiles) {
            for tile_y in unroll(0, warp_y_tiles) {
                let local_x = tile_x * tile_size_x;
                let local_y = tile_y * tile_size_y;

                let global_x = warp_x + local_x;
                let global_y = warp_y + local_y;

                let c_fragment_tensor = sub_tensor_f32(c, global_x, global_y, tile_size_x, tile_size_y);
                let c_fragment = ops.load_c(c_fragment_tensor);

                *acc_fragments(tile_x)(tile_y) = c_fragment;
            }
        }

        for global_k in range_step(0, k, tile_size_k) {
            //let mut a_fragments : [mat_a_type * 4];
            let mut a_fragment_1 : mat_a_type;
            let mut a_fragment_2 : mat_a_type;
            let mut a_fragment_3 : mat_a_type;
            let mut a_fragment_4 : mat_a_type;
            let a_fragments = @|y : i32| {
                match y {
                    0 => &mut a_fragment_1,
                    1 => &mut a_fragment_2,
                    2 => &mut a_fragment_3,
                    3 => &mut a_fragment_4,
                    _ => { pe_info("a fragment not found", y); &mut a_fragment_1 }
                }
            };

            for tile_x in unroll(0, warp_x_tiles /* = 2 */) {
                let local_x = tile_x * tile_size_x;
                let global_x = warp_x + local_x;

                let b_fragment_tensor = sub_tensor(b, global_x, global_k, tile_size_x, tile_size_k);
                let b_fragment = ops.load_b(b_fragment_tensor);

                for tile_y in unroll(0, warp_y_tiles /* = 4 */) {
                    let local_y = tile_y * tile_size_y;
                    let global_y = warp_y + local_y;

                    let acc_fragment = *acc_fragments(tile_x)(tile_y);

                    let a_fragment_tensor = sub_tensor(a, global_k, global_y, tile_size_k, tile_size_y);
                    let a_fragment = if (tile_x == 0) {
                        let a_fragment = ops.load_a(a_fragment_tensor);
                        *a_fragments(tile_y) = a_fragment;
                        a_fragment
                    } else {
                        *a_fragments(tile_y)
                    };

                    let result_fragment = ops.wmma (a_fragment, a_fragment_tensor.addr_mode, b_fragment, b_fragment_tensor.addr_mode, acc_fragment, c.addr_mode);

                    *acc_fragments(tile_x)(tile_y) = result_fragment;
                }
            }
        }

        for tile_x in unroll(0, warp_x_tiles) {
            for tile_y in unroll(0, warp_y_tiles) {
                let local_x = tile_x * tile_size_x;
                let local_y = tile_y * tile_size_y;

                let global_x = warp_x + local_x;
                let global_y = warp_y + local_y;

                let result_fragment = *acc_fragments(tile_x)(tile_y);

                let d_fragment_tensor = sub_tensor_f32(d, global_x, global_y, tile_size_x, tile_size_y);

                ops.store_d (d_fragment_tensor, result_fragment);
            }
        }
    }
}

/* Simple WMMA based implementation, does not include a fancy tiling strategy. */
fn matrix_multiply_acc_wmma [mat_a_type, mat_b_type, acc_type] (acc : Accelerator, ops : WMMAOperations[mat_a_type, mat_b_type, acc_type], a : Tensor, b : Tensor, c : Tensor_f32, d : Tensor_f32) -> () {
    let m = a.y_dim;
    let n = b.x_dim;
    //assert(a.x_dim == b.y_dim);
    let k = a.x_dim;

    let tile_size_x = 8;
    let tile_size_y = 8;
    let tile_size_k = 16;
    //let wave_size = ???;

    //We run blocks containing block_factor_x * block_factor_y many tiles.
    let block_factor_x = select(n % (tile_size_x * 2) != 0, 1, select(n % (tile_size_x * 4) != 0, 2, 4));
    let block_factor_y = select(m % (tile_size_y * 2) != 0, 1, select(m % (tile_size_y * 4) != 0, 2, 4));

    let matrix_x_threads = n * wave_size / tile_size_x;
    let matrix_y_threads = m / tile_size_y;

    let block_x_threads = wave_size * block_factor_x;
    let block_y_threads = block_factor_y;

    print_string("simple wmma configuration: (");
    print_i32(matrix_x_threads);
    print_string(", ");
    print_i32(matrix_y_threads);
    print_string(") block size (");
    print_i32(block_x_threads);
    print_string(", ");
    print_i32(block_y_threads);
    print_string(")\n");

    for work_item in acc.exec((matrix_x_threads, matrix_y_threads,  1), (block_x_threads, block_y_threads, 1)) {
        let tile_x = work_item.gidx() / wave_size * tile_size_x;
        let tile_y = work_item.gidy() * tile_size_y;

        let c_fragment_tensor = sub_tensor_f32(c, tile_x, tile_y, tile_size_x, tile_size_y);
        let mut acc_fragment = ops.load_c(c_fragment_tensor);

        for global_k in range_step(0, k, tile_size_k) {
            let a_fragment_tensor = sub_tensor(a, global_k, tile_y, tile_size_k, tile_size_y);
            let b_fragment_tensor = sub_tensor(b, tile_x, global_k, tile_size_x, tile_size_k);

            let a_fragment = ops.load_a(a_fragment_tensor);
            let b_fragment = ops.load_b(b_fragment_tensor);

            acc_fragment = ops.wmma(a_fragment, a_fragment_tensor.addr_mode, b_fragment, b_fragment_tensor.addr_mode, acc_fragment, c_fragment_tensor.addr_mode);
        }

        let d_fragment_tensor = sub_tensor_f32(d, tile_x, tile_y, tile_size_x, tile_size_y);
        ops.store_d(d_fragment_tensor, acc_fragment);
    }
}

/* Baseline implementation, does not use WMMA intrinsics. */
fn matrix_multiply_acc_simple [_mat_a_type, _mat_b_type, _acc_type] (acc : Accelerator, a : Tensor, b : Tensor, c : Tensor_f32, d : Tensor_f32) -> () {
    let m = a.y_dim;
    let n = b.x_dim;
    //assert(a.x_dim == b.y_dim);
    let k = a.x_dim;

    print_string("ref implementation\n");

    let block_size_x = select(n < 32, n, 32);
    let block_size_y = select(m < 16, m, 16);

    for work_item in acc.exec((n, m,  1), (block_size_x, block_size_y, 1)) {
        let x = work_item.gidx();
        let y = work_item.gidy();

        let mut rv = c.data(addr_tensor_f32(x, y, c));

        for i in range(0, k) {
            let av = a.data(addr_tensor(i, y, a)) as f32;
            let bv = b.data(addr_tensor(x, i, b)) as f32;

            rv += av * bv;
        }

        d.data(addr_tensor_f32(x, y, d)) = rv;
    }
}
