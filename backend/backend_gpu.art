/*
 * D = A * B + C
 *
 * Where:
 *
 * A:    (  -> K )
 *       (       )
 *       (| M    )
 *       (V      )
 *
 * B:    (  -> N )
 *       (       )
 *       (| K    )
 *       (V      )
 *
 * C, D: (  -> N )
 *       (       )
 *       (| M    )
 *       (V      )
 */

//Tested with ROCm 6.3.3
//TODO: This requires a patch in runtime:
/* in hsa_platform.cpp:616
-    std::string attrs = "-trap-handler";
+    std::string attrs = "-trap-handler,+wavefrontsize32,-wavefrontsize64";
*/

// This should never changeâ„¢.
static wave_size = 32;

struct WMMAOperations [mat_a_type, mat_b_type, acc_type] {
    load_a : fn (Tensor[a_element_type]) -> mat_a_type,
    load_b : fn (Tensor[b_element_type]) -> mat_b_type,
    load_c : fn (Tensor[c_element_type]) -> acc_type,
    wmma : fn (mat_a_type, AddrMode, mat_b_type, AddrMode, acc_type, AddrMode) -> acc_type,
    store_d : fn (Tensor[c_element_type], acc_type) -> ()
}

/* Blocked execution + caching A and B in shared memory to improve memory thoughput for loading these matricies in multiple warps of the same block during multiplication. */
fn matrix_multiply_acc_blocked [mat_a_type, mat_b_type, acc_type] (acc : Accelerator, ops : WMMAOperations[mat_a_type, mat_b_type, acc_type], a : Tensor[a_element_type], b : Tensor[b_element_type], c : Tensor[c_element_type], d : Tensor[c_element_type]) -> () {
    let m = a.y_dim;
    let n = b.x_dim;
    //assert(a.x_dim == b.y_dim);
    let k = a.x_dim;

    //Each warp will be used to calcuate this geometry of tiles.
    //let tile_size_x = 16;
    //let tile_size_y = 16;
    //let tile_size_k = 16;

    let warp_x_tiles = 2;
    let warp_y_tiles = 4;

    let block_x_warps = 4;
    let block_y_warps = 2;


    let block_x_tiles = block_x_warps * warp_x_tiles;
    let block_y_tiles = block_y_warps * warp_y_tiles;

    let block_size_x = block_x_tiles * tile_size_x;
    let block_size_y = block_y_tiles * tile_size_y;

    let wave_size_x = warp_x_tiles * tile_size_x;
    let wave_size_y = warp_y_tiles * tile_size_y;

    //Total matrix geometry
    let matrix_x_tiles = n / tile_size_x;
    let matrix_y_tiles = m / tile_size_y;

    let matrix_x_blocks = matrix_x_tiles / block_x_tiles;
    let matrix_y_blocks = matrix_y_tiles / block_y_tiles;

    let matrix_total_blocks = matrix_x_blocks * matrix_y_blocks;
    let block_total_warps = block_x_warps * block_y_warps;
    let threads_per_block = block_total_warps * wave_size;

    let parallel_blocks = device_mp_count;

    print_string("execute blocked ");
    print_string("tiles (");
    print_i32(warp_x_tiles);
    print_string(", ");
    print_i32(warp_y_tiles);
    print_string(") blocks (");
    print_i32(matrix_x_blocks);
    print_string(", ");
    print_i32(matrix_y_blocks);
    print_string(") launch (");
    print_i32(parallel_blocks);
    print_string(", ");
    print_i32(threads_per_block);
    print_string(")\n");

    let k_shared_tiles = 4;
    let skew_half = 16;
    let shared_a_size = (tile_size_k * k_shared_tiles + skew_half) * block_size_y;
    let shared_b_size = (tile_size_k * k_shared_tiles + skew_half) * block_size_x;

    for work_item in acc.exec((parallel_blocks * threads_per_block, 1, 1), (threads_per_block, 1, 1)) {
        let shared_memory_a = bitcast[&mut [a_element_type]](reserve_shared[a_element_type](shared_a_size));
        let shared_memory_b = bitcast[&mut [b_element_type]](reserve_shared[b_element_type](shared_b_size));

        let a_shared_tensor = Tensor[a_element_type] {
            data = shared_memory_a,
            x_dim = tile_size_k * k_shared_tiles,
            y_dim = block_size_y,
            addr_mode = AddrMode::RowMajor,
            stride = tile_size_k * k_shared_tiles + skew_half
        };
        let b_shared_tensor = Tensor[b_element_type] {
            data = shared_memory_b,
            x_dim = block_size_x,
            y_dim = tile_size_k * k_shared_tiles,
            addr_mode = AddrMode::ColMajor,
            //stride = block_size_x + skew_half
            stride = tile_size_k * k_shared_tiles + skew_half
        };

        let warp_id = work_item.tidx() / wave_size;
        let lane_id = work_item.tidx() % wave_size;

        for block_index in range_step(work_item.bidx(), matrix_total_blocks, parallel_blocks) {
            let block_x = (block_index % matrix_x_blocks) * block_size_x;
            let block_y = (block_index / matrix_x_blocks) * block_size_y;

            let warp_x = block_x + (warp_id % block_x_warps) * wave_size_x;
            let warp_y = block_y + (warp_id / block_x_warps) * wave_size_y;

            //TODO: this depends on the current layout!
            //Warning: [[acc_type * y] * x] and the corresponding accesses are not supported on the level zero backend, so lambdas are used for that.
            //Warning: The current solution requires the loops over tile_x and tile_y to be unrolled. Otherwise the program compiles and runs but results are incorrect ðŸ˜³
            //let mut acc_fragments : [[acc_type * 4 /* warp_y_tiles */] * 2 /* warp_x_tiles */];
            let mut acc_fragment_0 : acc_type;
            let mut acc_fragment_1 : acc_type;
            let mut acc_fragment_2 : acc_type;
            let mut acc_fragment_3 : acc_type;
            let mut acc_fragment_4 : acc_type;
            let mut acc_fragment_5 : acc_type;
            let mut acc_fragment_6 : acc_type;
            let mut acc_fragment_7 : acc_type;
            let acc_fragments = @|tile_x : i32| { @|tile_y : i32| {
                match (tile_x, tile_y) {
                    (0, 0) => &mut acc_fragment_0,
                    (0, 1) => &mut acc_fragment_1,
                    (0, 2) => &mut acc_fragment_2,
                    (0, 3) => &mut acc_fragment_3,
                    (1, 0) => &mut acc_fragment_4,
                    (1, 1) => &mut acc_fragment_5,
                    (1, 2) => &mut acc_fragment_6,
                    (1, 3) => &mut acc_fragment_7,
                    _ => &mut acc_fragment_0
                }
            }};

            /* Load C fragments into registers */
            for tile_x in unroll(0, warp_x_tiles) {
                for tile_y in unroll(0, warp_y_tiles) {
                    let local_x = tile_x * tile_size_x;
                    let local_y = tile_y * tile_size_y;

                    let global_x = warp_x + local_x;
                    let global_y = warp_y + local_y;

                    let c_fragment_tensor = sub_tensor(c, global_x, global_y, tile_size_x, tile_size_y);
                    let c_fragment = ops.load_c(c_fragment_tensor);

                    *acc_fragments(tile_x)(tile_y) = c_fragment;
                }
            }

            for global_k_sliced in range_step(0, k, tile_size_k * k_shared_tiles) { //TODO: That's l.295 in the sample, and should be unrolled!
                //Populate shared tensors
                //TODO: assumes a_element_type == b_element_type;
                let elements_per_operation = (sizeof[i32]() as i32 * 4) / (sizeof[a_element_type]() as i32);
                let operations_per_lane = (k_shared_tiles * tile_size_k) / elements_per_operation;
                let lanes_per_step = (wave_size * 4) / operations_per_lane;
                let lanes_per_warp = wave_size / operations_per_lane;

                if (warp_id < 4) { //tid 0 - 127
                    for y_index in unroll_step(0, block_size_y, lanes_per_step) {
                        let local_y = y_index + warp_id * lanes_per_warp + (lane_id / operations_per_lane);
                        let local_k = (lane_id % operations_per_lane) * elements_per_operation;

                        let global_y = block_y + local_y;
                        let global_k = global_k_sliced + local_k;

                        let src_ptr = bitcast[&simd[i32 * 4]](&a.data(addr_tensor(global_k, global_y, a)));
                        let target_ptr = bitcast[&mut simd[i32 * 4]](&mut a_shared_tensor.data(addr_tensor(local_k, local_y, a_shared_tensor)));
                        *target_ptr = *src_ptr;
                    }
                } else { //tid 128 - 255
                    for x_index in unroll_step(0, block_size_x, lanes_per_step) {
                        let local_x = x_index + (warp_id % 4) * lanes_per_warp + (lane_id / operations_per_lane);
                        let local_k = (lane_id % operations_per_lane) * elements_per_operation;

                        let global_x = block_x + local_x;
                        let global_k = global_k_sliced + local_k;

                        let src_ptr = bitcast[&simd[i32 * 4]](&b.data(addr_tensor(global_x, global_k, b)));
                        let target_ptr = bitcast[&mut simd[i32 * 4]](&mut b_shared_tensor.data(addr_tensor(local_x, local_k, b_shared_tensor)));
                        *target_ptr = *src_ptr;
                    }
                }

                acc.barrier();

                for local_k_tile in unroll(0, k_shared_tiles) {
                    let local_k = local_k_tile * tile_size_k;

                    //let mut a_fragments : [mat_a_type * 4 /* warp_y_tiles */];
                    let mut a_fragment_0 : mat_a_type;
                    let mut a_fragment_1 : mat_a_type;
                    let mut a_fragment_2 : mat_a_type;
                    let mut a_fragment_3 : mat_a_type;
                    let a_fragments = @|tile_y : i32| {
                        match tile_y {
                            0 => &mut a_fragment_0,
                            1 => &mut a_fragment_1,
                            2 => &mut a_fragment_2,
                            3 => &mut a_fragment_3,
                            _ => &mut a_fragment_0
                        }
                    };

                    for tile_x in unroll(0, warp_x_tiles /* 2 */) {
                        let local_x = tile_x * tile_size_x;
                        let global_x = warp_x + local_x;

                        let in_block_x = global_x - block_x;

                        let b_fragment_tensor = sub_tensor(b_shared_tensor, in_block_x, local_k, tile_size_x, tile_size_k);
                        let b_fragment = ops.load_b(b_fragment_tensor);

                        for tile_y in unroll(0, warp_y_tiles /* 4 */) {
                            let local_y = tile_y * tile_size_y;
                            let global_y = warp_y + local_y;

                            let in_block_y  = global_y - block_y;

                            let acc_fragment = *acc_fragments(tile_x)(tile_y);

                            let a_fragment_tensor = sub_tensor(a_shared_tensor, local_k, in_block_y, tile_size_k, tile_size_y);
                            if (tile_x == 0) {
                                *a_fragments(tile_y) = ops.load_a(a_fragment_tensor);
                            }
                            let a_fragment = *a_fragments(tile_y);

                            let result_fragment = ops.wmma (a_fragment, a_fragment_tensor.addr_mode, b_fragment, b_fragment_tensor.addr_mode, acc_fragment, c.addr_mode);

                            *acc_fragments(tile_x)(tile_y) = result_fragment;
                        }
                    }
                }

                acc.barrier();
            }

            for tile_x in unroll(0, warp_x_tiles) {
                for tile_y in unroll(0, warp_y_tiles) {
                    let local_x = tile_x * tile_size_x;
                    let local_y = tile_y * tile_size_y;

                    let global_x = warp_x + local_x;
                    let global_y = warp_y + local_y;

                    let result_fragment = *acc_fragments(tile_x)(tile_y);

                    let d_fragment_tensor = sub_tensor(d, global_x, global_y, tile_size_x, tile_size_y);

                    ops.store_d (d_fragment_tensor, result_fragment);
                }
            }
        }
    }
}

/* Similar to the tiled version, but reuses compute blocks to compute multiple matrix blocks. */
fn matrix_multiply_acc_blocked_noshm [mat_a_type, mat_b_type, acc_type] (acc : Accelerator, ops : WMMAOperations[mat_a_type, mat_b_type, acc_type], a : Tensor[a_element_type], b : Tensor[b_element_type], c : Tensor[c_element_type], d : Tensor[c_element_type]) -> () {
    let m = a.y_dim;
    let n = b.x_dim;
    //assert(a.x_dim == b.y_dim);
    let k = a.x_dim;

    //Each warp will be used to calcuate this geometry of tiles.
    //let tile_size_x = 16;
    //let tile_size_y = 16;
    //let tile_size_k = 16;

    let warp_x_tiles = 2;
    let warp_y_tiles = 4;

    let block_x_warps = 4;
    let block_y_warps = 2;


    let block_x_tiles = block_x_warps * warp_x_tiles;
    let block_y_tiles = block_y_warps * warp_y_tiles;

    let block_size_x = block_x_tiles * tile_size_x;
    let block_size_y = block_y_tiles * tile_size_y;

    let wave_size_x = warp_x_tiles * tile_size_x;
    let wave_size_y = warp_y_tiles * tile_size_y;

    //Total matrix geometry
    let matrix_x_tiles = n / tile_size_x;
    let matrix_y_tiles = m / tile_size_y;

    let matrix_x_blocks = matrix_x_tiles / block_x_tiles;
    let matrix_y_blocks = matrix_y_tiles / block_y_tiles;

    let matrix_total_blocks = matrix_x_blocks * matrix_y_blocks;
    let block_total_warps = block_x_warps * block_y_warps;
    let threads_per_block = block_total_warps * wave_size;

    let parallel_blocks = device_mp_count;

    print_string("execute blocked ");
    print_string("(");
    print_i32(matrix_x_tiles);
    print_string(", ");
    print_i32(matrix_y_tiles);
    print_string(") (");
    print_i32(matrix_x_blocks);
    print_string(", ");
    print_i32(matrix_y_blocks);
    print_string(") (");
    print_i32(parallel_blocks);
    print_string(", ");
    print_i32(threads_per_block);
    print_string(")\n");

    for work_item in acc.exec((parallel_blocks * threads_per_block, 1, 1), (threads_per_block, 1, 1)) {
        let warp_id = work_item.tidx() / wave_size;
        //let lane_id = work_item.tidx() % wave_size;

        for block_index in range_step(work_item.bidx(), matrix_total_blocks, parallel_blocks) {
            let block_x = (block_index % matrix_x_blocks) * block_size_x;
            let block_y = (block_index / matrix_x_blocks) * block_size_y;

            let warp_x = block_x + (warp_id % block_x_warps) * wave_size_x;
            let warp_y = block_y + (warp_id / block_x_warps) * wave_size_y;

            //TODO: this depends on the current layout!
            //Warning: [[acc_type * y] * x] and the corresponding accesses are not supported on the level zero backend, so lambdas are used for that.
            //Warning: The current solution requires the loops over tile_x and tile_y to be unrolled. Otherwise the program compiles and runs but results are incorrect ðŸ˜³
            //let mut acc_fragments : [[acc_type * 4 /* warp_y_tiles */] * 2 /* warp_x_tiles */];
            let mut acc_fragment_0 : acc_type;
            let mut acc_fragment_1 : acc_type;
            let mut acc_fragment_2 : acc_type;
            let mut acc_fragment_3 : acc_type;
            let mut acc_fragment_4 : acc_type;
            let mut acc_fragment_5 : acc_type;
            let mut acc_fragment_6 : acc_type;
            let mut acc_fragment_7 : acc_type;
            let acc_fragments = @|tile_x : i32| { @|tile_y : i32| {
                match (tile_x, tile_y) {
                    (0, 0) => &mut acc_fragment_0,
                    (0, 1) => &mut acc_fragment_1,
                    (0, 2) => &mut acc_fragment_2,
                    (0, 3) => &mut acc_fragment_3,
                    (1, 0) => &mut acc_fragment_4,
                    (1, 1) => &mut acc_fragment_5,
                    (1, 2) => &mut acc_fragment_6,
                    (1, 3) => &mut acc_fragment_7,
                    _ => &mut acc_fragment_0
                }
            }};

            /* Load C fragments into registers */
            for tile_x in unroll(0, warp_x_tiles) {
                for tile_y in unroll(0, warp_y_tiles) {
                    let local_x = tile_x * tile_size_x;
                    let local_y = tile_y * tile_size_y;

                    let global_x = warp_x + local_x;
                    let global_y = warp_y + local_y;

                    let c_fragment_tensor = sub_tensor(c, global_x, global_y, tile_size_x, tile_size_y);
                    let c_fragment = ops.load_c(c_fragment_tensor);

                    *acc_fragments(tile_x)(tile_y) = c_fragment;
                }
            }

            for global_k in range_step(0, k, tile_size_k) {
                //let mut a_fragments : [mat_a_type * 4 /* warp_y_tiles */];
                let mut a_fragment_0 : mat_a_type;
                let mut a_fragment_1 : mat_a_type;
                let mut a_fragment_2 : mat_a_type;
                let mut a_fragment_3 : mat_a_type;
                let a_fragments = @|tile_y : i32| {
                    match tile_y {
                        0 => &mut a_fragment_0,
                        1 => &mut a_fragment_1,
                        2 => &mut a_fragment_2,
                        3 => &mut a_fragment_3,
                        _ => &mut a_fragment_0
                    }
                };

                for tile_x in unroll(0, warp_x_tiles /* 2 */) {
                    let local_x = tile_x * tile_size_x;
                    let global_x = warp_x + local_x;

                    let b_fragment_tensor = sub_tensor(b, global_x, global_k, tile_size_x, tile_size_k);
                    let b_fragment = ops.load_b(b_fragment_tensor);

                    for tile_y in unroll(0, warp_y_tiles /* 4 */) {
                        let local_y = tile_y * tile_size_y;
                        let global_y = warp_y + local_y;

                        let acc_fragment = *acc_fragments(tile_x)(tile_y);

                        let a_fragment_tensor = sub_tensor(a, global_k, global_y, tile_size_k, tile_size_y);
                        if (tile_x == 0) {
                            *a_fragments(tile_y) = ops.load_a(a_fragment_tensor);
                        }
                        let a_fragment = *a_fragments(tile_y);

                        let result_fragment = ops.wmma (a_fragment, a_fragment_tensor.addr_mode, b_fragment, b_fragment_tensor.addr_mode, acc_fragment, c.addr_mode);

                        *acc_fragments(tile_x)(tile_y) = result_fragment;
                    }
                }
            }

            for tile_x in unroll(0, warp_x_tiles) {
                for tile_y in unroll(0, warp_y_tiles) {
                    let local_x = tile_x * tile_size_x;
                    let local_y = tile_y * tile_size_y;

                    let global_x = warp_x + local_x;
                    let global_y = warp_y + local_y;

                    let result_fragment = *acc_fragments(tile_x)(tile_y);

                    let d_fragment_tensor = sub_tensor(d, global_x, global_y, tile_size_x, tile_size_y);

                    ops.store_d (d_fragment_tensor, result_fragment)
                }
            }
        }
    }
}

/* Improved tiled version. We split the matrix in 16x16 tiles, and combine 8x8 such tiles to one block.
   Each block will then be computed using 8 warps. Consequently, each warp will compute 8 tiles. */
fn matrix_multiply_acc_tiled [mat_a_type, mat_b_type, acc_type] (acc : Accelerator, ops : WMMAOperations[mat_a_type, mat_b_type, acc_type], a : Tensor[a_element_type], b : Tensor[b_element_type], c : Tensor[c_element_type], d : Tensor[c_element_type]) -> () {
    let m = a.y_dim;
    let n = b.x_dim;
    //assert(a.x_dim == b.y_dim);
    let k = a.x_dim;

    //let tile_size_x = 16;
    //let tile_size_y = 16;
    //let tile_size_k = 16;

    //Each block will calculate x * y many tiles.
    let block_x_tiles = 8;
    let block_y_tiles = 8;

    //Each warp will be used to calculate this geometry of tiles.
    let warp_x_tiles = 2;
    let warp_y_tiles = 4;

    //Calculate the rest of the grid geometry
    let block_x_warps = block_x_tiles / warp_x_tiles;
    let block_y_warps = block_y_tiles / warp_y_tiles;

    let matrix_x_tiles = n / tile_size_x;
    let matrix_y_tiles = m / tile_size_y;

    let matrix_x_warps = matrix_x_tiles / warp_x_tiles;
    let matrix_y_warps = matrix_y_tiles / warp_y_tiles;

    let matrix_x_blocks = matrix_x_warps / block_x_warps;
    let matrix_y_blocks = matrix_y_warps / block_y_warps;

    let block_x_threads = block_x_warps * wave_size;
    let block_y_threads = block_y_warps;

    let matrix_x_threads = matrix_x_blocks * block_x_threads;
    let matrix_y_threads = matrix_y_blocks * block_y_threads;

    print_string("tiled configuration: (");
    print_i32(matrix_x_threads);
    print_string(", ");
    print_i32(matrix_y_threads);
    print_string(") block size (");
    print_i32(block_x_threads);
    print_string(", ");
    print_i32(block_y_threads);
    print_string(")\n");

    for work_item in acc.exec((matrix_x_threads, matrix_y_threads,  1), (block_x_threads, block_y_threads, 1)) {
        let warp_x = work_item.gidx() / wave_size * warp_x_tiles * tile_size_x;
        let warp_y = work_item.gidy() * warp_y_tiles * tile_size_y;

        //TODO: this depends on the current layout!
        //Warning: [[acc_type * y] * x] and the corresponding accesses are not supported on the level zero backend, so lambdas are used for that.
        //Warning: The current solution requires the loops over tile_x and tile_y to be unrolled. Otherwise the program compiles and runs but results are incorrect ðŸ˜³
        //let mut acc_fragments : [[acc_type * 4] * 2];
        let mut acc_fragment_0 : acc_type;
        let mut acc_fragment_1 : acc_type;
        let mut acc_fragment_2 : acc_type;
        let mut acc_fragment_3 : acc_type;
        let mut acc_fragment_4 : acc_type;
        let mut acc_fragment_5 : acc_type;
        let mut acc_fragment_6 : acc_type;
        let mut acc_fragment_7 : acc_type;
        let acc_fragments = @|tile_x : i32| { @|tile_y : i32| {
            match (tile_x, tile_y) {
                (0, 0) => &mut acc_fragment_0,
                (0, 1) => &mut acc_fragment_1,
                (0, 2) => &mut acc_fragment_2,
                (0, 3) => &mut acc_fragment_3,
                (1, 0) => &mut acc_fragment_4,
                (1, 1) => &mut acc_fragment_5,
                (1, 2) => &mut acc_fragment_6,
                (1, 3) => &mut acc_fragment_7,
                _ => &mut acc_fragment_0
            }
        }};

        /* Load C fragments into registers */
        for tile_x in unroll(0, warp_x_tiles) {
            for tile_y in unroll(0, warp_y_tiles) {
                let local_x = tile_x * tile_size_x;
                let local_y = tile_y * tile_size_y;

                let global_x = warp_x + local_x;
                let global_y = warp_y + local_y;

                let c_fragment_tensor = sub_tensor(c, global_x, global_y, tile_size_x, tile_size_y);
                let c_fragment = ops.load_c(c_fragment_tensor);

                *acc_fragments(tile_x)(tile_y) = c_fragment;
            }
        }

        for global_k in range_step(0, k, tile_size_k) {
            //let mut a_fragments : [mat_a_type * 4];
            let mut a_fragment_0 : mat_a_type;
            let mut a_fragment_1 : mat_a_type;
            let mut a_fragment_2 : mat_a_type;
            let mut a_fragment_3 : mat_a_type;
            let a_fragments = @|tile_y : i32| {
                match tile_y {
                    0 => &mut a_fragment_0,
                    1 => &mut a_fragment_1,
                    2 => &mut a_fragment_2,
                    3 => &mut a_fragment_3,
                    _ => &mut a_fragment_0
                }
            };

            for tile_x in unroll(0, warp_x_tiles /* = 2 */) {
                let local_x = tile_x * tile_size_x;
                let global_x = warp_x + local_x;

                let b_fragment_tensor = sub_tensor(b, global_x, global_k, tile_size_x, tile_size_k);
                let b_fragment = ops.load_b(b_fragment_tensor);

                for tile_y in unroll(0, warp_y_tiles /* = 4 */) {
                    let local_y = tile_y * tile_size_y;
                    let global_y = warp_y + local_y;

                    let acc_fragment = *acc_fragments(tile_x)(tile_y);

                    let a_fragment_tensor = sub_tensor(a, global_k, global_y, tile_size_k, tile_size_y);
                    if (tile_x == 0) {
                        *a_fragments(tile_y) = ops.load_a(a_fragment_tensor);
                    }
                    let a_fragment = *a_fragments(tile_y);

                    let result_fragment = ops.wmma (a_fragment, a_fragment_tensor.addr_mode, b_fragment, b_fragment_tensor.addr_mode, acc_fragment, c.addr_mode);

                    *acc_fragments(tile_x)(tile_y) = result_fragment;
                }
            }
        }

        for tile_x in unroll(0, warp_x_tiles) {
            for tile_y in unroll(0, warp_y_tiles) {
                let local_x = tile_x * tile_size_x;
                let local_y = tile_y * tile_size_y;

                let global_x = warp_x + local_x;
                let global_y = warp_y + local_y;

                let result_fragment = *acc_fragments(tile_x)(tile_y);

                let d_fragment_tensor = sub_tensor(d, global_x, global_y, tile_size_x, tile_size_y);

                ops.store_d (d_fragment_tensor, result_fragment);
            }
        }
    }
}

/* Simple WMMA based implementation, does not include a fancy tiling strategy. */
fn matrix_multiply_acc_wmma [mat_a_type, mat_b_type, acc_type] (acc : Accelerator, ops : WMMAOperations[mat_a_type, mat_b_type, acc_type], a : Tensor[a_element_type], b : Tensor[b_element_type], c : Tensor[c_element_type], d : Tensor[c_element_type]) -> () {
    let m = a.y_dim;
    let n = b.x_dim;
    //assert(a.x_dim == b.y_dim);
    let k = a.x_dim;

    //let tile_size_x = 16;
    //let tile_size_y = 16;
    //let tile_size_k = 16;

    //We run blocks containing block_factor_x * block_factor_y many tiles.
    let block_factor_x = select(n % (tile_size_x * 2) != 0, 1, select(n % (tile_size_x * 4) != 0, 2, 4));
    let block_factor_y = select(m % (tile_size_y * 2) != 0, 1, select(m % (tile_size_y * 4) != 0, 2, 4));

    let matrix_x_threads = n * wave_size / tile_size_x;
    let matrix_y_threads = m / tile_size_y;

    let block_x_threads = wave_size * block_factor_x;
    let block_y_threads = block_factor_y;

    print_string("simple wmma configuration: (");
    print_i32(matrix_x_threads);
    print_string(", ");
    print_i32(matrix_y_threads);
    print_string(") block size (");
    print_i32(block_x_threads);
    print_string(", ");
    print_i32(block_y_threads);
    print_string(")\n");

    for work_item in acc.exec((matrix_x_threads, matrix_y_threads,  1), (block_x_threads, block_y_threads, 1)) {
        let tile_x = work_item.gidx() / wave_size * tile_size_x;
        let tile_y = work_item.gidy() * tile_size_y;

        let c_fragment_tensor = sub_tensor(c, tile_x, tile_y, tile_size_x, tile_size_y);
        let mut acc_fragment = ops.load_c(c_fragment_tensor);

        for global_k in range_step(0, k, tile_size_k) {
            let a_fragment_tensor = sub_tensor(a, global_k, tile_y, tile_size_k, tile_size_y);
            let b_fragment_tensor = sub_tensor(b, tile_x, global_k, tile_size_x, tile_size_k);

            let a_fragment = ops.load_a(a_fragment_tensor);
            let b_fragment = ops.load_b(b_fragment_tensor);

            acc_fragment = ops.wmma(a_fragment, a_fragment_tensor.addr_mode, b_fragment, b_fragment_tensor.addr_mode, acc_fragment, c_fragment_tensor.addr_mode);
        }

        let d_fragment_tensor = sub_tensor(d, tile_x, tile_y, tile_size_x, tile_size_y);
        ops.store_d(d_fragment_tensor, acc_fragment);
    }
}

/* Baseline implementation, does not use WMMA intrinsics. */
fn matrix_multiply_acc_simple [_mat_a_type, _mat_b_type, _acc_type] (acc : Accelerator, a : Tensor[a_element_type], b : Tensor[b_element_type], c : Tensor[c_element_type], d : Tensor[c_element_type]) -> () {
    let m = a.y_dim;
    let n = b.x_dim;
    //assert(a.x_dim == b.y_dim);
    let k = a.x_dim;

    print_string("ref implementation\n");

    let block_size_x = select(n < 32, n, 32);
    let block_size_y = select(m < 16, m, 16);

    for work_item in acc.exec((n, m,  1), (block_size_x, block_size_y, 1)) {
        let x = work_item.gidx();
        let y = work_item.gidy();

        let mut rv = c.data(addr_tensor(x, y, c)) as f32;

        for i in range(0, k) {
            let av = a.data(addr_tensor(i, y, a)) as f32;
            let bv = b.data(addr_tensor(x, i, b)) as f32;

            rv += av * bv;
        }

        d.data(addr_tensor(x, y, d)) = rv as c_element_type;
    }
}
