/*
 * D = A * B + C
 *
 * Where:
 *
 * A:    (  -> K )
 *       (       )
 *       (| M    )
 *       (V      )
 *
 * B:    (  -> N )
 *       (       )
 *       (| K    )
 *       (V      )
 *
 * C, D: (  -> N )
 *       (       )
 *       (| M    )
 *       (V      )
 */

struct Tensor {
    data : &mut [f16],
    x_dim : i32,
    y_dim : i32,
    addr_mode : AddrMode,
    stride : i32 //row to row offset for this matrix.
}

fn @addr_tensor (x : i32, y : i32, t : Tensor) = match t.addr_mode {
    AddrMode::RowMayor => x + y * t.stride,
    AddrMode::ColMayor => x * t.stride + y
};

fn @sub_tensor(t : Tensor, x0 : i32, y0 : i32, xs : i32, ys : i32) =
    Tensor {data = bitcast [&mut [f16]](&t.data(addr_tensor(x0, y0, t))),
             x_dim = xs,
             y_dim = ys,
             addr_mode = t.addr_mode,
             stride = t.stride
             };

fn print_matrix (t : Tensor) -> () {
    for y in range(0, t.y_dim) {
        for x in range(0, t.x_dim) {
            print_f32(t.data(addr_tensor(x, y, t)) as f32);
            if (x < t.x_dim - 1) { print_string(", "); }
        }
        print_string("\n");
    }
}


fn matrix_multiply_naive (a : Tensor, b : Tensor, c : Tensor, r : Tensor) -> () {
    let m = a.y_dim;
    let n = b.x_dim;
    //assert(a.x_dim == b.y_dim);
    let k = a.x_dim;

    for y in range(0, m) {
        for x in range(0, n) {
            let mut rv = 0 : f32;

            for i in range(0, k) {
                let av = a.data(addr_tensor(i, y, a)) as f32;
                let bv = b.data(addr_tensor(x, i, b)) as f32;

                rv += av * bv;
            }

            let cv = c.data(addr_tensor(x, y, c)) as f32;

            r.data(addr_tensor(x, y, r)) = (cv + rv) as f16;
        }
    }
}
