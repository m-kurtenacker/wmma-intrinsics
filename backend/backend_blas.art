/*
 * D = A * B + C
 *
 * Where:
 *
 * A:    (  -> K )
 *       (       )
 *       (| M    )
 *       (V      )
 *
 * B:    (  -> N )
 *       (       )
 *       (| K    )
 *       (V      )
 *
 * C, D: (  -> N )
 *       (       )
 *       (| M    )
 *       (V      )
 */

struct convert_function[T] { convert : fn (T) -> f32 }
implicit = convert_function[f16] { convert = @| a | { a as f32 } };
implicit = convert_function[f32] { convert = @| a | { a } };
implicit = convert_function[f64] { convert = @| a | { a as f32 } };

fn convert_tensor[T] (src : Tensor[T], implicit convert : convert_function[T]) -> Tensor[f32] {
    let dst_cpu = bitcast[&mut[f32]](alloc_cpu(sizeof[f32]() * src.x_dim as i64 * src.y_dim as i64).data);

    let dst = Tensor[f32] { data = dst_cpu, x_dim = src.x_dim, y_dim = src.y_dim, addr_mode = src.addr_mode, stride = src.stride };

    for x in range(0, src.x_dim) {
        for y in range(0, src.y_dim) {
            dst.data(addr_tensor(x, y, dst)) = convert.convert(src.data(addr_tensor(x, y, src)));
        }
    }

    dst
}

//Warning: This is only legal if S == T. But it has to exist for compatibility reasons.
fn @id_convert[S, T] (src : Tensor[S]) = Tensor[T] {
    data = bitcast[&mut [T]](src.data),
    x_dim = src.x_dim,
    y_dim = src.y_dim,
    addr_mode = src.addr_mode,
    stride = src.stride
};

fn matrix_multiply_blas (a : Tensor[a_element_type], b : Tensor[b_element_type], c : Tensor[c_element_type]) -> () {
    let m = a.y_dim as i64;
    let n = b.x_dim as i64;
    //assert(a.x_dim == b.y_dim);
    let k = a.x_dim as i64;

    let a_f32 = if (sizeof[a_element_type]() == 4 : i64) { //if a_element_type == f32, but worse.
        id_convert[a_element_type, f32](a)
    } else {
        convert_tensor[a_element_type](a)
    };
    let b_f32 = if (sizeof[b_element_type]() == 4 : i64) {
        id_convert[b_element_type, f32](b)
    } else {
        convert_tensor[b_element_type](b)
    };
    let c_f32 = if (sizeof[c_element_type]() == 4 : i64) {
        id_convert[c_element_type, f32](c)
    } else {
        convert_tensor[c_element_type](c)
    };

    cblas_gemm[f32](CBLAS_LAYOUT::CblasRowMajor, CBLAS_TRANSPOSE::CblasNoTrans, CBLAS_TRANSPOSE::CblasTrans, m, n, k, 1.0, bitcast[&mut [f32]](a_f32.data), a_f32.stride as i64, bitcast[&mut [f32]](b_f32.data), b_f32.stride as i64, 1.0, bitcast[&mut [f32]](c_f32.data), c_f32.stride as i64);

    if (sizeof[c_element_type]() != 4 : i64) {
        for x in range(0, c_f32.x_dim) {
            for y in range(0, c_f32.y_dim) {
                c.data(addr_tensor(x, y, c)) = c_f32.data(addr_tensor(x, y, c_f32)) as c_element_type;
            }
        }
    }
}
