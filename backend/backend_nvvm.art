/*
 * D = A * B + C
 *
 * Where:
 *
 * A:    (  -> K )
 *       (       )
 *       (| M    )
 *       (V      )
 *
 * B:    (  -> N )
 *       (       )
 *       (| K    )
 *       (V      )
 *
 * C, D: (  -> N )
 *       (       )
 *       (| M    )
 *       (V      )
 */

type nvvm_acc_datatype = (simd[f16 * 2], simd[f16 * 2], simd[f16 * 2], simd[f16 * 2]);
type nvvm_mat_datatype = (simd[f16 * 2], simd[f16 * 2], simd[f16 * 2], simd[f16 * 2], simd[f16 * 2], simd[f16 * 2], simd[f16 * 2], simd[f16 * 2]);

fn @nvvm_wmma_expand (a_fragment : nvvm_mat_datatype, b_fragment : nvvm_mat_datatype, c_fragment : nvvm_acc_datatype, a_mode : AddrMode, b_mode : AddrMode) -> nvvm_acc_datatype {
    let (a1, a2, a3, a4, a5, a6, a7, a8) = a_fragment;
    let (b1, b2, b3, b4, b5, b6, b7, b8) = b_fragment;
    let (c1, c2, c3, c4) = c_fragment;

    match (a_mode, b_mode) {
        (AddrMode::RowMayor, AddrMode::RowMayor) =>
            nvvm_wmma_row_row(a1, a2, a3, a4, a5, a6, a7, a8, b1, b2, b3, b4, b5, b6, b7, b8, c1, c2, c3, c4),
        (AddrMode::ColMayor, AddrMode::RowMayor) =>
            nvvm_wmma_col_row(a1, a2, a3, a4, a5, a6, a7, a8, b1, b2, b3, b4, b5, b6, b7, b8, c1, c2, c3, c4),
        (AddrMode::RowMayor, AddrMode::ColMayor) =>
            nvvm_wmma_row_col(a1, a2, a3, a4, a5, a6, a7, a8, b1, b2, b3, b4, b5, b6, b7, b8, c1, c2, c3, c4),
        (AddrMode::ColMayor, AddrMode::ColMayor) =>
            nvvm_wmma_col_col(a1, a2, a3, a4, a5, a6, a7, a8, b1, b2, b3, b4, b5, b6, b7, b8, c1, c2, c3, c4)
    }
}

fn @nvvm_wmma_load_a_expand(fragment_tensor : Tensor) -> nvvm_mat_datatype {
    let cuda_data = bitcast[&i8](fragment_tensor.data);

    match fragment_tensor.addr_mode {
        AddrMode::RowMayor =>
            if (fragment_tensor.stride > 16) {
                nvvm_wmma_load_a_row_stride(cuda_data, fragment_tensor.stride)
            } else {
                nvvm_wmma_load_a_row(cuda_data)
            },
        AddrMode::ColMayor =>
            if (fragment_tensor.stride > 16) {
                nvvm_wmma_load_a_col_stride(cuda_data, fragment_tensor.stride)
            } else {
                nvvm_wmma_load_a_col(cuda_data)
            }
    }
}

fn @nvvm_wmma_load_b_expand(fragment_tensor : Tensor) -> nvvm_mat_datatype {
    let cuda_data = bitcast[&i8](fragment_tensor.data);

    match fragment_tensor.addr_mode {
        AddrMode::RowMayor =>
            if (fragment_tensor.stride > 16) {
                nvvm_wmma_load_b_row_stride(cuda_data, fragment_tensor.stride)
            } else {
                nvvm_wmma_load_b_row(cuda_data)
            },
        AddrMode::ColMayor =>
            if (fragment_tensor.stride > 16) {
                nvvm_wmma_load_b_col_stride(cuda_data, fragment_tensor.stride)
            } else {
                nvvm_wmma_load_b_col(cuda_data)
            }
    }
}

fn @nvvm_wmma_load_c_expand(fragment_tensor : Tensor) -> nvvm_acc_datatype {
    let cuda_data = bitcast[&i8](fragment_tensor.data);

    match fragment_tensor.addr_mode {
        AddrMode::RowMayor =>
            if (fragment_tensor.stride > 16) {
                nvvm_wmma_load_c_row_stride(cuda_data, fragment_tensor.stride)
            } else {
                nvvm_wmma_load_c_row(cuda_data)
            },
        AddrMode::ColMayor =>
            if (fragment_tensor.stride > 16) {
                nvvm_wmma_load_c_col_stride(cuda_data, fragment_tensor.stride)
            } else {
                nvvm_wmma_load_c_col(cuda_data)
            }
    }
}

fn @nvvm_wmma_store_d_expand (d_fragment_tensor : Tensor, acc_fragment : nvvm_acc_datatype) -> () {
    let d_cuda = bitcast[&mut i8](d_fragment_tensor.data);
    let (acc1, acc2, acc3, acc4) = acc_fragment;

    match d_fragment_tensor.addr_mode {
        AddrMode::RowMayor =>
            if (d_fragment_tensor.stride > 16) {
                nvvm_wmma_store_d_row_stride(d_cuda, acc1, acc2, acc3, acc4, d_fragment_tensor.stride);
            } else {
                nvvm_wmma_store_d_row(d_cuda, acc1, acc2, acc3, acc4);
            },
        AddrMode::ColMayor =>
            if (d_fragment_tensor.stride > 16) {
                nvvm_wmma_store_d_col_stride(d_cuda, acc1, acc2, acc3, acc4, d_fragment_tensor.stride);
            } else {
                nvvm_wmma_store_d_col(d_cuda, acc1, acc2, acc3, acc4);
            }
    }
}

static NVVMWMMAOperations = WMMAOperations [nvvm_mat_datatype, nvvm_acc_datatype] {
    load_a = @|fragment_tensor| {
        nvvm_wmma_load_a_expand(fragment_tensor)
    },
    load_b = @|fragment_tensor| {
        nvvm_wmma_load_b_expand(fragment_tensor)
    },
    load_c = @|fragment_tensor| {
        nvvm_wmma_load_c_expand(fragment_tensor)
    },
    wmma = @|a_fragment, a_layout, b_fragment, b_layout, acc_fragment, _c_layout| {
        nvvm_wmma_expand (a_fragment, b_fragment, acc_fragment, a_layout, b_layout)
    },
    store_d = @|fragment_tensor, acc_fragment| {
        nvvm_wmma_store_d_expand (fragment_tensor, acc_fragment);
    }
};
