/*
 * D = A * B + C
 *
 * Where:
 *
 * A:    (  -> K )
 *       (       )
 *       (| M    )
 *       (V      )
 *
 * B:    (  -> N )
 *       (       )
 *       (| K    )
 *       (V      )
 *
 * C, D: (  -> N )
 *       (       )
 *       (| M    )
 *       (V      )
 */

// This should never changeâ„¢.
static warp_size = 32;


fn @nvvm_wmma_expand (a_fragment : (simd[f16 * 2], simd[f16 * 2], simd[f16 * 2], simd[f16 * 2], simd[f16 * 2], simd[f16 * 2], simd[f16 * 2], simd[f16 * 2]),
        b_fragment : (simd[f16 * 2], simd[f16 * 2], simd[f16 * 2], simd[f16 * 2], simd[f16 * 2], simd[f16 * 2], simd[f16 * 2], simd[f16 * 2]),
        c_fragment : (simd[f16 * 2], simd[f16 * 2], simd[f16 * 2], simd[f16 * 2]),
        a_mode : AddrMode,
        b_mode : AddrMode,
        ) {
    let (a1, a2, a3, a4, a5, a6, a7, a8) = a_fragment;
    let (b1, b2, b3, b4, b5, b6, b7, b8) = b_fragment;
    let (c1, c2, c3, c4) = c_fragment;

    match (a_mode, b_mode) {
        (AddrMode::RowMayor, AddrMode::RowMayor) =>
            nvvm_wmma_row_row(a1, a2, a3, a4, a5, a6, a7, a8, b1, b2, b3, b4, b5, b6, b7, b8, c1, c2, c3, c4),
        (AddrMode::ColMayor, AddrMode::RowMayor) =>
            nvvm_wmma_col_row(a1, a2, a3, a4, a5, a6, a7, a8, b1, b2, b3, b4, b5, b6, b7, b8, c1, c2, c3, c4),
        (AddrMode::RowMayor, AddrMode::ColMayor) =>
            nvvm_wmma_row_col(a1, a2, a3, a4, a5, a6, a7, a8, b1, b2, b3, b4, b5, b6, b7, b8, c1, c2, c3, c4),
        (AddrMode::ColMayor, AddrMode::ColMayor) =>
            nvvm_wmma_col_col(a1, a2, a3, a4, a5, a6, a7, a8, b1, b2, b3, b4, b5, b6, b7, b8, c1, c2, c3, c4)
    }
}

fn @nvvm_load_a_expand(fragment_tensor : Tensor) {
    let cuda_data = bitcast[&i8](fragment_tensor.data);

    match fragment_tensor.addr_mode {
        AddrMode::RowMayor =>
            if (fragment_tensor.stride > 16) {
                nvvm_load_a_row_stride(cuda_data, fragment_tensor.stride)
            } else {
                nvvm_load_a_row(cuda_data)
            },
        AddrMode::ColMayor =>
            if (fragment_tensor.stride > 16) {
                nvvm_load_a_col_stride(cuda_data, fragment_tensor.stride)
            } else {
                nvvm_load_a_col(cuda_data)
            }
    }
}

fn @nvvm_load_b_expand(fragment_tensor : Tensor) {
    let cuda_data = bitcast[&i8](fragment_tensor.data);

    match fragment_tensor.addr_mode {
        AddrMode::RowMayor =>
            if (fragment_tensor.stride > 16) {
                nvvm_load_b_row_stride(cuda_data, fragment_tensor.stride)
            } else {
                nvvm_load_b_row(cuda_data)
            },
        AddrMode::ColMayor =>
            if (fragment_tensor.stride > 16) {
                nvvm_load_b_col_stride(cuda_data, fragment_tensor.stride)
            } else {
                nvvm_load_b_col(cuda_data)
            }
    }
}

fn @nvvm_load_c_expand(fragment_tensor : Tensor) {
    let cuda_data = bitcast[&i8](fragment_tensor.data);

    match fragment_tensor.addr_mode {
        AddrMode::RowMayor =>
            if (fragment_tensor.stride > 16) {
                nvvm_load_c_row_stride(cuda_data, fragment_tensor.stride)
            } else {
                nvvm_load_c_row(cuda_data)
            },
        AddrMode::ColMayor =>
            if (fragment_tensor.stride > 16) {
                nvvm_load_c_col_stride(cuda_data, fragment_tensor.stride)
            } else {
                nvvm_load_c_col(cuda_data)
            }
    }
}

fn @nvvm_store_d_expand (d_fragment_tensor : Tensor, acc_fragment : (simd[f16 * 2], simd[f16 * 2], simd[f16 * 2], simd[f16 * 2])) {
    let d_cuda = bitcast[&mut i8](d_fragment_tensor.data);
    let (acc1, acc2, acc3, acc4) = acc_fragment;

    match d_fragment_tensor.addr_mode {
        AddrMode::RowMayor =>
            if (d_fragment_tensor.stride > 16) {
                nvvm_store_d_row_stride(d_cuda, acc1, acc2, acc3, acc4, d_fragment_tensor.stride)
            } else {
                nvvm_store_d_row(d_cuda, acc1, acc2, acc3, acc4)
            },
        AddrMode::ColMayor =>
            if (d_fragment_tensor.stride > 16) {
                nvvm_store_d_col_stride(d_cuda, acc1, acc2, acc3, acc4, d_fragment_tensor.stride)
            } else {
                nvvm_store_d_col(d_cuda, acc1, acc2, acc3, acc4)
            }
    }
}


fn matrix_multiply_nvvm (nvvm : Accelerator, a : Tensor, b : Tensor, c : Tensor, d : Tensor) -> () {
    let m = a.y_dim;
    let n = b.x_dim;
    //assert(a.x_dim == b.y_dim);
    let k = a.x_dim;

    let chunk_size_x = 16;
    let chunk_size_y = 16;
    let chunk_size_k = 16;

    let block_factor_x = select(n % (chunk_size_x * 2) != 0, 1, select(n % (chunk_size_x * 4) != 0, 2, 4));
    let block_factor_y = select(m % (chunk_size_y * 2) != 0, 1, select(m % (chunk_size_y * 4) != 0, 2, 4));

    for work_item in nvvm.exec((n * warp_size / chunk_size_x, m / chunk_size_y,  1), (warp_size * block_factor_x, block_factor_y, 1)) { //We spawn (x * y * warp_size many threads in warp_size many blocks
        let chunk_x = work_item.gidx() / warp_size * chunk_size_x;
        let chunk_y = work_item.gidy() * chunk_size_y;

        let c_fragment_tensor = sub_tensor(c, chunk_x, chunk_y, chunk_size_x, chunk_size_y);
        let mut acc_fragment = nvvm_load_c_expand(c_fragment_tensor);

        for i in range_step(0, k, chunk_size_k) {
            //let id = [nvvm_warpid() as i32, i, chunk_x, chunk_y];
            //if (nvvm_laneid() == 0) { nvvm_vprintf("%d %d %d %d\n", bitcast[&[u8]](&id)); }

            let a_fragment_tensor = sub_tensor(a, i, chunk_y, chunk_size_k, chunk_size_y);
            let b_fragment_tensor = sub_tensor(b, chunk_x, i, chunk_size_x, chunk_size_k);

            let a_fragment = nvvm_load_a_expand(a_fragment_tensor);
            let b_fragment = nvvm_load_b_expand(b_fragment_tensor);

            acc_fragment = nvvm_wmma_expand (a_fragment, b_fragment, acc_fragment, a_fragment_tensor.addr_mode, b_fragment_tensor.addr_mode);
        }

        let d_fragment_tensor = sub_tensor(d, chunk_x, chunk_y, chunk_size_x, chunk_size_y);
        nvvm_store_d_expand (d_fragment_tensor, acc_fragment)
    }
}
