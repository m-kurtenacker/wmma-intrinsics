/*
 * D = A * B + C
 *
 * Where:
 *
 * A:    (  -> K )
 *       (       )
 *       (| M    )
 *       (V      )
 *
 * B:    (  -> N )
 *       (       )
 *       (| K    )
 *       (V      )
 *
 * C, D: (  -> N )
 *       (       )
 *       (| M    )
 *       (V      )
 */

// This should never changeâ„¢.
static warp_size = 32;

struct Tensor {
    data : &mut [f16],
    x_dim : i32,
    y_dim : i32,
    stride : i32 //row to row offset for this matrix.
}

struct Tensor1 {
    data : &mut addrspace(1)[f16],
    x_dim : i32,
    y_dim : i32,
    stride : i32 //row to row offset for this matrix.
}

fn @addr_row (x : i32, y : i32, t : Tensor) = x + y * t.stride;
fn @addr_row1 (x : i32, y : i32, t : Tensor1) = x + y * t.stride;

fn @sub_tensor(t : Tensor, x0 : i32, y0 : i32, xs : i32, ys : i32) =
    Tensor {data = bitcast [&mut [f16]](&t.data(addr_row(x0, y0, t))),
             x_dim = xs,
             y_dim = ys,
             stride = t.stride
             };

fn @sub_tensor1 (t : Tensor1, x0 : i32, y0 : i32, xs : i32, ys : i32) =
    Tensor1 {data = bitcast [&mut addrspace(1)[f16]](&t.data(addr_row1(x0, y0, t))),
             x_dim = xs,
             y_dim = ys,
             stride = t.stride
             };

fn print_matrix (t : Tensor) -> () {
    for y in range(0, t.y_dim) {
        for x in range(0, t.x_dim) {
            print_f32(t.data(addr_row(x, y, t)) as f32);
            if (x < t.x_dim - 1) { print_string(", "); }
        }
        print_string("\n");
    }
}


fn matrix_multiply_naive (a : Tensor, b : Tensor, c : Tensor, r : Tensor) -> () {
    let m = a.y_dim;
    let n = b.x_dim;
    //assert(a.x_dim == b.y_dim);
    let k = a.x_dim;

    for y in range(0, m) {
        for x in range(0, n) {
            let mut rv = 0 : f32;

            for i in range(0, k) {
                let av = a.data(addr_row(i, y, a)) as f32;
                let bv = b.data(addr_row(x, i, b)) as f32;

                rv += av * bv;
            }

            let cv = c.data(addr_row(x, y, c)) as f32;

            r.data(addr_row(x, y, r)) = (cv + rv) as f16;
        }
    }
}


fn nvvm_gemm_base (a : Tensor1, b : Tensor1, c : Tensor1, d : Tensor1) -> () {
    let a_cuda = bitcast[&addrspace(1)i8](a.data);
    let b_cuda = bitcast[&addrspace(1)i8](b.data);
    let c_cuda = bitcast[&addrspace(1)i8](c.data);
    let d_cuda = bitcast[&mut addrspace(1)i8](d.data);

    let (a1, a2, a3, a4, a5, a6, a7, a8) = if (a.stride > 16) { nvvm_load_a_stride(a_cuda, a.stride) } else { nvvm_load_a(a_cuda) };
    let (b1, b2, b3, b4, b5, b6, b7, b8) = if (b.stride > 16) { nvvm_load_b_stride(b_cuda, b.stride) } else { nvvm_load_b(b_cuda) };
    let (c1, c2, c3, c4) = if (c.stride > 16) { nvvm_load_c_stride(c_cuda, c.stride) } else { nvvm_load_c(c_cuda) };

    let (d1, d2, d3, d4) = nvvm_wmma (a1, a2, a3, a4, a5, a6, a7, a8,
                                      b1, b2, b3, b4, b5, b6, b7, b8,
                                      c1, c2, c3, c4);

    if (d.stride > 16) { nvvm_store_d_stride(d_cuda, d1, d2, d3, d4, d.stride) } else { nvvm_store_d(d_cuda, d1, d2, d3, d4) };
}

fn matrix_multiply_nvvm (nvvm : Accelerator, a : Tensor1, b : Tensor1, c : Tensor1, d : Tensor1) -> () {
    let m = a.y_dim;
    let n = b.x_dim;
    //assert(a.x_dim == b.y_dim);
    let k = a.x_dim;

    let chunk_size_x = 16;
    let chunk_size_y = 16;
    let chunk_size_k = 16;

    for work_item in nvvm.exec((n * warp_size / chunk_size_x, m / chunk_size_y,  1), (warp_size, 1, 1)) { //We spawn (x * y * warp_size many threads in warp_size many blocks
        for i in range(0, k / chunk_size_k) {
            let warp_id = nvvm_warpid() as i32;
            let lane_id = nvvm_laneid() as i32;

            let chunk_x = work_item.gidx() / warp_size * chunk_size_x;
            let chunk_y = work_item.gidy() * chunk_size_y;

            let debug = [ warp_id, chunk_x, chunk_y, i ];
            if (lane_id == 0) {
                nvvm_vprintf("%d %d %d %d\n", bitcast [&[u8]](&debug));
            }

            let a_fragment_tensor = sub_tensor1(a, i * chunk_size_k, chunk_y, chunk_size_k, chunk_size_y);
            let b_fragment_tensor = sub_tensor1(b, chunk_x, i * chunk_size_k, chunk_size_x, chunk_size_k);

            let d_fragment_tensor = sub_tensor1(d, chunk_x, chunk_y, chunk_size_x, chunk_size_y);
            let c_fragment_tensor = if(i == 0) { sub_tensor1(c, chunk_x, chunk_y, chunk_size_x, chunk_size_y) } else { d_fragment_tensor };

            nvvm_gemm_base(a_fragment_tensor, b_fragment_tensor, c_fragment_tensor, d_fragment_tensor);
        }
    }
}


#[export]
fn main (_argc : i32, _argv : &[&[u8]]) -> i32 {
    let nvvm = nvvm_accelerator(0);

    let N = 32;
    let M = 32;
    let K = 32;

    // Produce data set on host
    let a_cpu_buffer = alloc_cpu(sizeof[f16]() * (K * M) as i64);
    let b_cpu_buffer = alloc_cpu(sizeof[f16]() * (N * K) as i64);
    let c_cpu_buffer = alloc_cpu(sizeof[f16]() * (N * M) as i64);
    let d_cpu_buffer = alloc_cpu(sizeof[f16]() * (N * M) as i64);
    let r_cpu_buffer = alloc_cpu(sizeof[f16]() * (N * M) as i64);

    let a_cpu = bitcast[&mut [f16]](a_cpu_buffer.data);
    let b_cpu = bitcast[&mut [f16]](b_cpu_buffer.data);
    let c_cpu = bitcast[&mut [f16]](c_cpu_buffer.data);
    let d_cpu = bitcast[&mut [f16]](d_cpu_buffer.data);
    let r_cpu = bitcast[&mut [f16]](r_cpu_buffer.data);

    for n in range(0, M * K) { a_cpu(n) = random_val_f32() as f16; }
    for n in range(0, N * K) { b_cpu(n) = random_val_f32() as f16; } 
    for n in range(0, N * M) { c_cpu(n) = random_val_f32() as f16; d_cpu(n) = 0 as f16; r_cpu(n) = 0 as f16; }

    // Copy data to device
    let a_cuda_buffer = nvvm.alloc(sizeof[f16]() * (K * M) as i64);
    let b_cuda_buffer = nvvm.alloc(sizeof[f16]() * (N * K) as i64);
    let c_cuda_buffer = nvvm.alloc(sizeof[f16]() * (N * M) as i64);
    let d_cuda_buffer = nvvm.alloc(sizeof[f16]() * (N * M) as i64);

    copy(a_cpu_buffer, a_cuda_buffer);
    copy(b_cpu_buffer, b_cuda_buffer);
    copy(c_cpu_buffer, c_cuda_buffer);

    let a_global_tensor = Tensor1 { data = bitcast[&mut addrspace(1)[f16]](a_cuda_buffer.data), x_dim = K, y_dim = M, stride = K };
    let b_global_tensor = Tensor1 { data = bitcast[&mut addrspace(1)[f16]](b_cuda_buffer.data), x_dim = N, y_dim = K, stride = N };
    let c_global_tensor = Tensor1 { data = bitcast[&mut addrspace(1)[f16]](c_cuda_buffer.data), x_dim = N, y_dim = M, stride = N };
    let d_global_tensor = Tensor1 { data = bitcast[&mut addrspace(1)[f16]](d_cuda_buffer.data), x_dim = N, y_dim = M, stride = N };

    // NVVM multiply tensors
    matrix_multiply_nvvm(nvvm, a_global_tensor, b_global_tensor, c_global_tensor, d_global_tensor);

    // Fetch result
    copy(d_cuda_buffer, d_cpu_buffer);

    // Ref implementation
    let a_tensor = Tensor { data = a_cpu, x_dim = K, y_dim = M, stride = K };
    let b_tensor = Tensor { data = b_cpu, x_dim = N, y_dim = K, stride = N };
    let c_tensor = Tensor { data = c_cpu, x_dim = N, y_dim = M, stride = N };
    let r_tensor = Tensor { data = r_cpu, x_dim = N, y_dim = M, stride = N };
    matrix_multiply_naive(a_tensor, b_tensor, c_tensor, r_tensor);

    // Compare results
    let fin_tensor = Tensor { data = r_cpu, x_dim = N, y_dim = M, stride = N };
    let d_tensor = Tensor { data = d_cpu, x_dim = N, y_dim = M, stride = N };

    print_string("Fin matrix:\n");
    print_matrix(fin_tensor);
    print_string("\n");
    print_string("D matrix:\n");
    print_matrix(d_tensor);
    print_string("\n");

    for x in range(0, d_tensor.x_dim) {
        for y in range(0, d_tensor.y_dim) {
            fin_tensor.data(addr_row(x, y, fin_tensor)) = fin_tensor.data(addr_row(x, y, fin_tensor)) - d_tensor.data(addr_row(x, y, d_tensor));
        }
    }

    print_string("Difference:\n");
    print_matrix(fin_tensor);
    print_string("\n");

    let mut max_error = 0 as f32;
    for x in range(0, N * M) {
        max_error = cpu_intrinsics.fmaxf(max_error, cpu_intrinsics.fabsf(r_cpu(x) as f32));
    }

    if (max_error < 0.01) {
        0
    } else {
        print_string("\n");
        print_string("Max error: ");
        print_f32(max_error);
        print_string("\n");
        1
    }
}
